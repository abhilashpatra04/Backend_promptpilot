# MCP Server LLM - Environment Variables Template
# Copy this file to .env and fill in your actual values
# NEVER commit .env files to version control!

# ==================== REQUIRED ====================

# Google Generative AI (Gemini)
# Get from: https://ai.google.dev/tutorials/setup
GOOGLE_API_KEY=your_google_generative_ai_api_key_here

# Path to Firebase service account JSON file
# Download from Firebase Console > Project Settings > Service Accounts
GOOGLE_APPLICATION_CREDENTIALS=./firebase-service-account.json

# Firebase Project ID
FIREBASE_PROJECT_ID=your_firebase_project_id

# ==================== OPTIONAL ====================

# Groq API (for fast LLM inference)
# Get from: https://console.groq.com
GROQ_API_KEY=your_groq_api_key_here

# OpenRouter API (for multiple model access)
# Get from: https://openrouter.ai
OPENROUTER_API_KEY=your_openrouter_api_key_here

# Cloudinary (for file upload management)
# Get from: https://cloudinary.com
CLOUDINARY_CLOUD_NAME=your_cloudinary_cloud_name
CLOUDINARY_API_KEY=your_cloudinary_api_key
CLOUDINARY_API_SECRET=your_cloudinary_api_secret

# ==================== SERVER CONFIGURATION ====================

# Server host (default: 0.0.0.0)
SERVER_HOST=0.0.0.0

# Server port (default: 8000)
SERVER_PORT=8000

# Enable CORS for frontend (comma-separated URLs)
CORS_ORIGINS=http://localhost:3000,http://localhost:8000

# ==================== LOGGING ====================

# Log level (DEBUG, INFO, WARNING, ERROR, CRITICAL)
LOG_LEVEL=INFO

# ==================== FEATURE FLAGS ====================

# Enable streaming responses
ENABLE_STREAMING=true

# Enable PDF processing
ENABLE_PDF_PROCESSING=true

# Enable web search
ENABLE_WEB_SEARCH=true

# ==================== ADVANCED ====================

# Tesseract OCR path (if not in PATH)
# Windows: C:\\Program Files\\Tesseract-OCR\\tesseract.exe
# macOS/Linux: /usr/bin/tesseract
PYTESSERACT_PATH=

# Vector store directory
VECTOR_STORE_DIR=./vectorstores

# Maximum request size (in MB)
MAX_REQUEST_SIZE=50

# Request timeout (in seconds)
REQUEST_TIMEOUT=300

# ==================== MODELS ====================

# Default LLM model
DEFAULT_MODEL=gemini-1.5-pro

# Available models
# Google: gemini-1.5-pro, gemini-1.5-flash, gemini-pro
# Groq: mixtral-8x7b-32768, llama2-70b-4096
# OpenRouter: varies by provider

# ==================== DATABASE ====================

# Firestore timeout (in seconds)
FIRESTORE_TIMEOUT=30

# Message history limit per chat
MESSAGE_HISTORY_LIMIT=50
